{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwE8dClSGfbV",
        "outputId": "df7173c5-d630-4791-ea90-ef5b7a9ef6e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Tokenizer\n",
            "  Downloading tokenizer-3.4.2-py2.py3-none-any.whl (79 kB)\n",
            "     ---------------------------------------- 79.1/79.1 kB 1.5 MB/s eta 0:00:00\n",
            "Installing collected packages: Tokenizer\n",
            "Successfully installed Tokenizer-3.4.2\n",
            "\n",
            "[notice] A new release of pip available: 22.1.2 -> 22.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SRJi40yJKOb",
        "outputId": "7491a896-1eab-4801-a034-6cbf555e7876"
      },
      "outputs": [],
      "source": [
        "!pip install pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge3RzdcoI4Xu"
      },
      "outputs": [],
      "source": [
        "import tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLiBJf72EXK",
        "outputId": "1d07a000-5c52-4d38-eaae-ccd44c55909b"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Embedding,LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7KNbRfY4wUK",
        "outputId": "1b3a16ba-2b55-4bf1-d6ac-9c2e1bb7d8da"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86w7Mhhw2R9V"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UNDDh3l32RtI",
        "outputId": "434351f5-2019-4caf-c1c9-17d64aaea0a0"
      },
      "outputs": [],
      "source": [
        "data=data.drop(['discourse_id','essay_id'],axis='columns')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SduvMbB2ZJ9",
        "outputId": "014900f7-2596-4105-e342-bcc07e995f03"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glpi9Ltw2Y-O",
        "outputId": "2e5a5aa0-4491-4a33-9a26-784dbb29717f"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "IiJKipoU2feq",
        "outputId": "b7e63224-dc76-4ee3-c607-3db18bfb75f0"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2zPB6GaG2fU8",
        "outputId": "9cb9b3f7-c77f-46b2-c37c-51abfccb927e"
      },
      "outputs": [],
      "source": [
        "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
        "data = data.replace(new_label)\n",
        "data = data.rename(columns = {\"discourse_effectiveness\": \"label\"})\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KSR5Cwgh2qTX",
        "outputId": "2e9b4543-59d2-4d23-8435-b41fe778504f"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdKfbujs2qP9"
      },
      "outputs": [],
      "source": [
        "def gen_freq(text):\n",
        "    #will store all the words in list\n",
        "    words_list = []\n",
        "    \n",
        "    #Loop over all the words and extract word from list\n",
        "    for word in text.split():\n",
        "        words_list.extend(word)\n",
        "        \n",
        "    #Generate word frequencies using value counts in word_list\n",
        "    word_freq = pd.Series(words_list).value_counts()\n",
        "    \n",
        "    #print top 100 words\n",
        "    word_freq[:100]\n",
        "    \n",
        "    return word_freq   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY4hMJK62qM8",
        "outputId": "ba8dd4b5-1cb2-44d0-c525-dd1af93c29bb"
      },
      "outputs": [],
      "source": [
        "freq = gen_freq(data.discourse_text.str)\n",
        "freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXW5bDmh3BHh"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_word_list = stopwords.words('english')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-JFv17L3CmW"
      },
      "outputs": [],
      "source": [
        "tokenizer=ToktokTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWvGYd8w3CeG"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_word_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stop_word_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "data['discourse_text']= data['discourse_text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULiyFtWJ3CRe"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#clearing punctuation & unnecessary marks\n",
        "data['discourse_text'] = data['discourse_text'].apply(lambda x: re.sub('[,\\.!?:()\"]', '', x))\n",
        "data['discourse_text'] = data['discourse_text'].apply(lambda x: re.sub('[^a-zA-Z\"]', ' ', x))\n",
        "\n",
        "#capitalization to lowercase\n",
        "data['discourse_text'] = data['discourse_text'].apply(lambda x: x.lower())\n",
        "\n",
        "#cleaning extra spaces\n",
        "data['discourse_text'] = data['discourse_text'].apply(lambda x: x.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wwh1v4z3CFx"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob210iZd3Qoy"
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "data['discourse_text']=data['discourse_text'].apply(denoise_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAIzl1Qp3Qb_"
      },
      "outputs": [],
      "source": [
        "data_copy = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEHwJBf73YTc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEJeP6_v3YKB",
        "outputId": "5df6e65d-d1cb-49e6-f400-655cc0bb0833"
      },
      "outputs": [],
      "source": [
        "label = data['label'].values\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZuHG0TlKYFF",
        "outputId": "fb9d97fe-9a65-4f94-c557-8b2dd21144cc"
      },
      "outputs": [],
      "source": [
        "data['label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqyUJU1o3jD2"
      },
      "outputs": [],
      "source": [
        "data = data['discourse_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuuzQiIg3i5z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(data,label,test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBk65PEm3iuo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 15000)\n",
        "tokenizer.fit_on_texts(data)\n",
        "#tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IBm_Rl93pPQ"
      },
      "outputs": [],
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsE0dkum3pDR"
      },
      "outputs": [],
      "source": [
        "#Then we take the word count of each of our sentences in our data and create a list.\n",
        "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
        "num_tokens = np.array(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVQMsXkE3w-8",
        "outputId": "05be1d04-00ea-49d9-a258-a5126c30989e"
      },
      "outputs": [],
      "source": [
        "#Here, when setting the number of tokens, a number is determined by taking into account the variability around the average.\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diGqOzWP3wzQ",
        "outputId": "edf3979c-2465-4b94-cd62-054a3a5c8d31"
      },
      "outputs": [],
      "source": [
        "#It is checked what percentage of the data this determined number covers.\n",
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quXJnE1R35fF",
        "outputId": "af7c99d5-c654-4676-ef82-73af2deb1b46"
      },
      "outputs": [],
      "source": [
        "#data is adjusted according to the number of tokens specified\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\n",
        "x_train_pad.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyhyoTcc35Tp"
      },
      "outputs": [],
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "\n",
        "def return_to_sentence(tokens):\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\n",
        "    text = ' '.join(words)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vea4AMH35Jc",
        "outputId": "050dbd8f-476b-49bb-d038-1ce00f774c3a"
      },
      "outputs": [],
      "source": [
        "#normal comment\n",
        "print(return_to_sentence(x_train_pad[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTfCdmLC4GSP",
        "outputId": "a713277a-140d-4101-d66f-4b5a95c159d9"
      },
      "outputs": [],
      "source": [
        "#token equivalent of comment\n",
        "print(x_train_pad[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE1Ik6_I4GG5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.optimizer_v2.rmsprop import RMSProp\n",
        "model = Sequential()\n",
        "\n",
        "embedding_size = 50\n",
        "\n",
        "model.add(Embedding(input_dim=15000,output_dim=embedding_size,input_length=max_tokens,name='embedding_layer'))\n",
        "\n",
        "model.add(LSTM(units=16, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(LSTM(units=8, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(LSTM(units=4))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "optimizer = RMSProp()\n",
        "\n",
        "model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip5-uiaN4Ns9",
        "outputId": "dfdab8d5-2088-4425-b25b-c847ff729c6c"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQfv6zoj4F8X",
        "outputId": "2e23b0bd-c017-48b7-d3c1-042a71a63dac"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train_pad, y_train, validation_split=0.3, epochs=90, batch_size=100, shuffle=True, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI8HPqtN4Fx0",
        "outputId": "d22ce5fa-23b9-4e18-e71d-c03acce12b4e"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTj_TxVA4Yi6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcR_Kl3ZA-Xp"
      },
      "source": [
        "**BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOhs2GA-A_Lb"
      },
      "outputs": [],
      "source": [
        "train, test= train_test_split(data_copy, test_size=0.2, random_state=42)\n",
        "Xtrain, ytrain = train['discourse_text'], train['label']\n",
        "Xtest, ytest = test['discourse_text'], test['label']\n",
        "#splitting the train set into train and validation\n",
        "Xtrain,Xval,ytrain,yval=train_test_split(Xtrain,ytrain,test_size=0.2,random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYIstrlS4YUI",
        "outputId": "ea4eec65-acdd-4369-ebe1-e03dfde629c7"
      },
      "outputs": [],
      "source": [
        "#set up the tokenizer\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "tk = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\n",
        "tk.fit_on_texts(Xtrain)\n",
        "word_index = tk.word_index\n",
        "#print(word_index)\n",
        "V = len(word_index)\n",
        "print(\"Vocabulary of the dataset is : \",V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCCow4a6BehN"
      },
      "outputs": [],
      "source": [
        "##create sequences of reviews\n",
        "seq_train = tk.texts_to_sequences(Xtrain)\n",
        "seq_test =  tk.texts_to_sequences(Xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDVCwfnQBeX0",
        "outputId": "3610b547-9ffb-43b4-d293-d56e474fff08"
      },
      "outputs": [],
      "source": [
        "#choice of maximum length of sequences\n",
        "seq_len_list = [len(i) for i in seq_train + seq_test]\n",
        "\n",
        "#if we take the direct maximum then\n",
        "max_len=max(seq_len_list)\n",
        "print('Maximum length of sequence in the list: {}'.format(max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0LLjLMGBeO-",
        "outputId": "6dcf8749-77ac-4be4-fd60-835fb65a05c3"
      },
      "outputs": [],
      "source": [
        "# when setting the maximum length of sequence, variability around the average is used.\n",
        "max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n",
        "max_seq_len = int(max_seq_len)\n",
        "print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V-tiMenEKfC",
        "outputId": "4324bddf-31a0-4391-b860-8ee583b7a642"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6arhWc8GBzIl"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "#Perform tokenization\n",
        "# automatically download the vocab used during pretraining or fine-tuning a given model,use from_pretrained() method\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIJrX04mBy8Y"
      },
      "outputs": [],
      "source": [
        "#pass our texts to the tokenizer. \n",
        "Xtrain_enc = tokenizer(Xtrain.tolist(), max_length=max_seq_len, \n",
        "                         truncation=True, padding='max_length', \n",
        "                         add_special_tokens=True, return_tensors='np') #return numpy object\n",
        "Xval_enc = tokenizer(Xval.tolist(), max_length=max_seq_len, \n",
        "                         truncation=True, padding='max_length', \n",
        "                         add_special_tokens=True, return_tensors='np') #return numpy object\n",
        "Xtest_enc = tokenizer(Xtest.tolist(), max_length=max_seq_len, \n",
        "                         truncation=True, padding='max_length', \n",
        "                         add_special_tokens=True, return_tensors='np') #return numpy object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE3UUawpB6gx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#preparing our datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(Xtrain_enc),\n",
        "    ytrain\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(Xval_enc),\n",
        "    yval\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(Xtest_enc),\n",
        "    ytest\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA4FmScLB6Xu"
      },
      "outputs": [],
      "source": [
        "# creating BERT Model\n",
        "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D\n",
        "from tensorflow.keras.models import Model\n",
        "def bert_model(train_dataset,val_dataset,transformer,max_len,epochs):\n",
        "    print(\"----Building the model----\")\n",
        "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = Input(shape=(max_len,),dtype=tf.int32,name = 'attention_mask') #attention mask\n",
        "    sequence_output = transformer(input_ids,attention_mask)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    x = Dense(512, activation='softmax')(cls_token)\n",
        "    x = Dropout(0.1)(x)\n",
        "    y = Dense(3, activation='relu')(x)\n",
        "    model = Model(inputs=[input_ids,attention_mask], outputs=y)\n",
        "    model.summary()\n",
        "    model.compile(Adam(lr=2e-5), loss='rmsprop', metrics=['accuracy'])\n",
        "    r = model.fit(train_dataset.batch(32),batch_size = 32,\n",
        "                  validation_data = val_dataset.batch(32),epochs = epochs)\n",
        "                  #callbacks = callbacks\n",
        "    print(\"Train score:\", model.evaluate(train_dataset.batch(32)))\n",
        "    print(\"Validation score:\", model.evaluate(val_dataset.batch(32)))\n",
        "    n_epochs = len(r.history['loss'])\n",
        "    \n",
        "    return r,model,n_epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akVn5IKYCLIL",
        "outputId": "40662690-9505-467d-aea2-d701d3351613"
      },
      "outputs": [],
      "source": [
        "transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8l65KnQkCLAK",
        "outputId": "f7ee8e02-9fd0-4a10-ce91-2387be3ef774"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "max_len = max_seq_len\n",
        "r,model,n_epochs = bert_model(train_dataset,val_dataset,transformer,max_len,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfP5yHJbCKwN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f40167f4cee62d5ec502cee6af97d3e18af8d66ead8f1a8e4861fdd3bb77419f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
